{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "import json, argparse, os\n",
    "import re\n",
    "import io\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataPath = \"train.txt\"\n",
    "testDataPath = \"devwithoutlabels.txt\"\n",
    "# Output file that will be generated. This file can be directly submitted.\n",
    "solutionPath = \"test.txt\"\n",
    "# Path to directory where GloVe file is saved.\n",
    "gloveDir = \"./\"\n",
    "NUM_FOLDS = 5                   # Value of K in K-fold Cross Validation\n",
    "NUM_CLASSES = 4                 # Number of classes - Happy, Sad, Angry, Others\n",
    "MAX_NB_WORDS = 20000                # To set the upper limit on the number of tokens extracted using keras.preprocessing.text.Tokenizer \n",
    "MAX_SEQUENCE_LENGTH = 100         # All sentences having lesser number of words than this will be padded\n",
    "EMBEDDING_DIM = 300               # The dimension of the word embeddings\n",
    "BATCH_SIZE = 200                  # The batch size to be chosen for training the model.\n",
    "LSTM_DIM = 128                    # The dimension of the representations learnt by the LSTM model\n",
    "DROPOUT = 0.2  \n",
    "LEARNING_RATE = 0.003 # Fraction of the units to drop for the linear transformation of the inputs. Ref - https://keras.io/layers/recurrent/\n",
    "NUM_EPOCHS = 75                  # Number of epochs to train a model for\n",
    "'''\n",
    "\n",
    "Change parameters here for the LSTM model used below\n",
    "\n",
    "'''\n",
    "\n",
    "label2emotion = {0:\"others\", 1:\"happy\", 2: \"sad\", 3:\"angry\"}\n",
    "emotion2label = {\"others\":0, \"happy\":1, \"sad\":2, \"angry\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(predictions, ground):\n",
    "    \"\"\"Given predicted labels and the respective ground truth labels, display some metrics\n",
    "    Input: shape [# of samples, NUM_CLASSES]\n",
    "        predictions : Model output. Every row has 4 decimal values, with the highest belonging to the predicted class\n",
    "        ground : Ground truth labels, converted to one-hot encodings. A sample belonging to Happy class will be [0, 1, 0, 0]\n",
    "    Output:\n",
    "        accuracy : Average accuracy\n",
    "        microPrecision : Precision calculated on a micro level. Ref - https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/16001\n",
    "        microRecall : Recall calculated on a micro level\n",
    "        microF1 : Harmonic mean of microPrecision and microRecall. Higher value implies better classification  \n",
    "    \"\"\"\n",
    "    # [0.1, 0.3 , 0.2, 0.1] -> [0, 1, 0, 0]\n",
    "    discretePredictions = to_categorical(predictions.argmax(axis=1))\n",
    "    \n",
    "    truePositives = np.sum(discretePredictions*ground, axis=0)\n",
    "    falsePositives = np.sum(np.clip(discretePredictions - ground, 0, 1), axis=0)\n",
    "    falseNegatives = np.sum(np.clip(ground-discretePredictions, 0, 1), axis=0)\n",
    "    '''\n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    \n",
    "    IMPORTANT\n",
    "    Please add  code here or somewhere else to print the TruePositives,FalsePositives,FalseNegatives\n",
    "    \n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    '''\n",
    "    \n",
    "    print(\"True Positives per class : \", truePositives)\n",
    "    print(\"False Positives per class : \", falsePositives)\n",
    "    print(\"False Negatives per class : \", falseNegatives)\n",
    "    \n",
    "    # ------------- Macro level calculation ---------------\n",
    "    macroPrecision = 0\n",
    "    macroRecall = 0\n",
    "    # We ignore the \"Others\" class during the calculation of Precision, Recall and F1\n",
    "    for c in range(1, NUM_CLASSES):\n",
    "        precision = truePositives[c] / (truePositives[c] + falsePositives[c])\n",
    "        macroPrecision += precision\n",
    "        recall = truePositives[c] / (truePositives[c] + falseNegatives[c])\n",
    "        macroRecall += recall\n",
    "        f1 = ( 2 * recall * precision ) / (precision + recall) if (precision+recall) > 0 else 0\n",
    "        print(\"Class %s : Precision : %.3f, Recall : %.3f, F1 : %.3f\" % (label2emotion[c], precision, recall, f1))\n",
    "    \n",
    "    macroPrecision /= 3\n",
    "    macroRecall /= 3\n",
    "    macroF1 = (2 * macroRecall * macroPrecision ) / (macroPrecision + macroRecall) if (macroPrecision+macroRecall) > 0 else 0\n",
    "    print(\"Ignoring the Others class, Macro Precision : %.4f, Macro Recall : %.4f, Macro F1 : %.4f\" % (macroPrecision, macroRecall, macroF1))   \n",
    "    \n",
    "    # ------------- Micro level calculation ---------------\n",
    "    truePositives = truePositives[1:].sum()\n",
    "    falsePositives = falsePositives[1:].sum()\n",
    "    falseNegatives = falseNegatives[1:].sum()    \n",
    "    \n",
    "    print(\"Ignoring the Others class, Micro TP : %d, FP : %d, FN : %d\" % (truePositives, falsePositives, falseNegatives))\n",
    "    \n",
    "    microPrecision = truePositives / (truePositives + falsePositives)\n",
    "    microRecall = truePositives / (truePositives + falseNegatives)\n",
    "    \n",
    "    microF1 = ( 2 * microRecall * microPrecision ) / (microPrecision + microRecall) if (microPrecision+microRecall) > 0 else 0\n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    ground = ground.argmax(axis=1)\n",
    "    accuracy = np.mean(predictions==ground)\n",
    "    \n",
    "    print(\"Accuracy : %.4f, Micro Precision : %.4f, Micro Recall : %.4f, Micro F1 : %.4f\" % (accuracy, microPrecision, microRecall, microF1))\n",
    "    return accuracy, microPrecision, microRecall, microF1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeNormalisedData(dataFilePath, texts):\n",
    "    \"\"\"Write normalised data to a file\n",
    "    Input:\n",
    "        dataFilePath : Path to original train/test file that has been processed\n",
    "        texts : List containing the normalised 3 turn conversations, separated by the <eos> tag.\n",
    "    \"\"\"\n",
    "    \n",
    "    '''\n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    \n",
    "    You May ignore this function\n",
    "    \n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    '''\n",
    "    normalisedDataFilePath = dataFilePath.replace(\".txt\", \"_normalised.txt\")\n",
    "    with io.open(normalisedDataFilePath, 'w', encoding='utf8') as fout:\n",
    "        with io.open(dataFilePath, encoding='utf8') as fin:\n",
    "            fin.readline()\n",
    "            for lineNum, line in enumerate(fin):\n",
    "                line = line.strip().split('\\t')\n",
    "                normalisedLine = texts[lineNum].strip().split('<eos>')\n",
    "                fout.write(line[0] + '\\t')\n",
    "                # Write the original turn, followed by the normalised version of the same turn\n",
    "                fout.write(line[1] + '\\t' + normalisedLine[0] + '\\t')\n",
    "                fout.write(line[2] + '\\t' + normalisedLine[1] + '\\t')\n",
    "                fout.write(line[3] + '\\t' + normalisedLine[2] + '\\t')\n",
    "                try:\n",
    "                    # If label information available (train time)\n",
    "                    fout.write(line[4] + '\\n')    \n",
    "                except:\n",
    "                    # If label information not available (test time)\n",
    "                    fout.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    ##########################################################\\n    ##########################################################\\n    \\n    Fasttext model being loaded here. It might take 1 min to load.\\n    This bin file is of 10mb you can look for 5 gb variant of it.\\n    Also download the 300d English language '.bin' file not '.vec' file.\\n    \\n    ##########################################################\\n    ##########################################################\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.wrappers import FastText\n",
    "mod = FastText.load_fasttext_format('/home/bt1/17CS10037/taddhita/cc.en.300.bin', encoding=\"utf8\")\n",
    "\n",
    "\n",
    "'''\n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    \n",
    "    Fasttext model being loaded here. It might take 1 min to load.\n",
    "    This bin file is of 10mb you can look for 5 gb variant of it.\n",
    "    Also download the 300d English language '.bin' file not '.vec' file.\n",
    "    \n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix(wordIndex):\n",
    "    \"\"\"Populate an embedding matrix using a word-index. If the word \"happy\" has an index 19,\n",
    "       the 19th row in the embedding matrix should contain the embedding vector for the word \"happy\".\n",
    "    Input:\n",
    "        wordIndex : A dictionary of (word : index) pairs, extracted using a tokeniser\n",
    "    Output:\n",
    "        embeddingMatrix : A matrix where every row has 100 dimensional GloVe embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    # Minimum word index of any word is 1. \n",
    "    '''\n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    \n",
    "    If you want to use gloVe model you may use but main purpose of it is to generate wordtovec\n",
    "    If possible look up for some better library than fasttext.bin file here which can deal with emoji as well\n",
    "    \n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    '''\n",
    "    i=0\n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, EMBEDDING_DIM))\n",
    "    for word, i in wordIndex.items():\n",
    "        try:\n",
    "            embeddingVector = mod[word]\n",
    "            embeddingMatrix[i] = embeddingVector    \n",
    "        except KeyError:\n",
    "            print(word)\n",
    "            i=i+1\n",
    "    print(i)\n",
    "    return embeddingMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import regex\n",
    "\n",
    "def split_count(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ğŸ˜', 'ğŸ˜']\n"
     ]
    }
   ],
   "source": [
    "line = [\"ğŸ¤” ğŸ™ˆ me asÃ­, se ğŸ˜Œ ds ğŸ’•ğŸ‘­ğŸ‘™ hello ğŸ‘©ğŸ¾â€ğŸ“ emoji hello ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ how are ğŸ˜Š you todayğŸ™…ğŸ½ğŸ™…ğŸ½\"]\n",
    "line1=[\"money money and lots of moneyğŸ˜ğŸ˜\"]\n",
    "\n",
    "counter = split_count(line1[0])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(dataFilePath, mode):\n",
    "    \"\"\"Load data from a file, process and return indices, conversations and labels in separate lists\n",
    "    Input:\n",
    "        dataFilePath : Path to train/test file to be processed\n",
    "        mode : \"train\" mode returns labels. \"test\" mode doesn't return labels.\n",
    "    Output:\n",
    "        indices : Unique conversation ID list\n",
    "        conversations : List of 3 turn conversations, processed and each turn separated by the <eos> tag\n",
    "        labels : [Only available in \"train\" mode] List of labels\n",
    "    \"\"\"\n",
    "    \n",
    "    '''\n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    \n",
    "    IMPORTANT\n",
    "    Please try to better the pre-processing here by sepearting the emojis with text like  \"hmmğŸ˜\", \"nowâ€¼â€¼â€¼ğŸ˜£ğŸ˜¤ğŸ˜¤\" & etc.\n",
    "    Also look for other methods.\n",
    "    \n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    '''\n",
    "    indices = []\n",
    "    conversations = []\n",
    "    labels = []\n",
    "    i=0\n",
    "    with io.open(dataFilePath, encoding=\"utf8\") as finput:\n",
    "        finput.readline()\n",
    "        for line in finput:\n",
    "            i+=1\n",
    "            # Convert multiple instances of . ? ! , to single instance\n",
    "            # okay...sure -> okay . sure\n",
    "            # okay???sure -> okay ? sure\n",
    "            # Add whitespace around such punctuation\n",
    "            # okay!sure -> okay ! sure\n",
    "            emojis = split_count(line)\n",
    "            repeatedChars = ['.']\n",
    "            repeatedChars.append('?')\n",
    "            repeatedChars.append(',')\n",
    "                           \n",
    "            for c in repeatedChars:\n",
    "                lineSplit = line.split(c)\n",
    "                while True:\n",
    "                    try:\n",
    "                        lineSplit.remove('')\n",
    "                    except:\n",
    "                        break\n",
    "                cSpace = ' ' + c + ' '    \n",
    "                line = cSpace.join(lineSplit)\n",
    "            for c in emojis:\n",
    "                lineSplit = line.split(c)\n",
    "                while True:\n",
    "                    try:\n",
    "                        lineSplit.remove('')\n",
    "                    except:\n",
    "                        break\n",
    "                cSpace = ' ' + c + ' '    \n",
    "                line = cSpace.join(lineSplit)\n",
    "            \n",
    "            line = line.strip().split('\\t')\n",
    "            if mode == \"train\":\n",
    "                # Train data contains id, 3 turns and label\n",
    "                label = emotion2label[line[4]]\n",
    "                labels.append(label)\n",
    "            \n",
    "            conv = ' <eos> '.join(line[1:4])\n",
    "            \n",
    "            # Remove any duplicate spaces\n",
    "            duplicateSpacePattern = re.compile(r'\\ +')\n",
    "            conv = re.sub(duplicateSpacePattern, ' ', conv)\n",
    "            \n",
    "            indices.append(int(line[0]))\n",
    "            conversations.append(conv.lower())\n",
    "    if mode == \"train\":\n",
    "        return indices, conversations, labels\n",
    "    else:\n",
    "        return indices, conversations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Processing test data...\n",
      "Extracting tokens...\n",
      "Found 15111 unique tokens.\n",
      "Populating embedding matrix...\n",
      "ğŸ˜‚\n",
      "ğŸ˜­\n",
      "i'll\n",
      "ğŸ˜\n",
      "ğŸ˜\n",
      "ğŸ˜¢\n",
      "ğŸ˜\n",
      "ğŸ˜€\n",
      "ğŸ˜¡\n",
      "ğŸ˜…\n",
      "i've\n",
      "ğŸ˜†\n",
      "ğŸ˜„\n",
      "ğŸ˜Š\n",
      "ğŸ˜ƒ\n",
      "ğŸ˜’\n",
      "ğŸ˜ \n",
      "ğŸ™‚\n",
      "ğŸ˜¤\n",
      "ğŸ˜‰\n",
      "ğŸ˜˜\n",
      "ğŸ˜Œ\n",
      "ğŸ˜¹\n",
      "ğŸ˜º\n",
      "how's\n",
      "ğŸ˜©\n",
      "i'd\n",
      "ğŸ˜¸\n",
      "ğŸ˜«\n",
      "ğŸ‘\n",
      "iâ€™m\n",
      "ğŸ’”\n",
      "ğŸ˜»\n",
      "ğŸ˜½\n",
      "ğŸ˜‘\n",
      "ğŸ˜‹\n",
      "ğŸ˜\n",
      "ğŸ˜”\n",
      "â¤ï¸\n",
      "ğŸ˜\n",
      "ğŸ™\n",
      "ğŸ˜¬\n",
      "ain't\n",
      "ğŸ˜¾\n",
      "ğŸ™€\n",
      "ğŸ˜¿\n",
      "ğŸ˜§\n",
      "ğŸ˜¦\n",
      "ğŸ™„\n",
      "ğŸ˜›\n",
      "ğŸ‘\n",
      "ğŸ˜\n",
      "ğŸ˜–\n",
      "ğŸ˜¥\n",
      "ğŸ\n",
      "ğŸ¤£\n",
      "ğŸ˜‡\n",
      "â€‘d\n",
      "ğŸ˜•\n",
      "â˜ºï¸\n",
      "ğŸ’•\n",
      "ğŸ˜ª\n",
      "ğŸ˜£\n",
      "'â€‘\n",
      "ğŸ‘Œ\n",
      "ğŸ¤”\n",
      "ğŸ˜¶\n",
      "ğŸ˜Ÿ\n",
      "ğŸ™ˆ\n",
      "ğŸ™\n",
      "ğŸ˜±\n",
      "ğŸ˜³\n",
      "iÌ‡\n",
      "8â€‘d\n",
      "xâ€‘d\n",
      "ğŸ˜¯\n",
      "ğŸ˜°\n",
      "ğŸ’™\n",
      "ğŸ˜“\n",
      "ğŸ˜¨\n",
      "ğŸ˜®\n",
      "okh\n",
      "ğŸ’‹\n",
      "ğŸ¤—\n",
      "ğŸ˜ˆ\n",
      "ğŸ’\n",
      "how're\n",
      "ğŸ–•\n",
      "â™‚ï¸\n",
      "ğŸ˜´\n",
      "ó¾Œ¹\n",
      "ğŸ‘„\n",
      "âœ¨\n",
      "ğŸ’š\n",
      "it'll\n",
      "ğŸ˜·\n",
      "â€‘c\n",
      "iâ€™ve\n",
      "ğŸ‘\n",
      "ğŸ’–\n",
      "â€‘3\n",
      "ğŸ‘€\n",
      "ï¼šï¼‰\n",
      "ãƒ»Ï‰ãƒ»\n",
      "u'r\n",
      "ğŸ’—\n",
      "ohhk\n",
      "ğŸ‘¦\n",
      "ğŸ‘¿\n",
      "ğŸ˜š\n",
      "ğŸ’\n",
      "byyy\n",
      "ğŸ’˜\n",
      "ğŸ‘Š\n",
      "it'd\n",
      "nthg\n",
      "hbu\n",
      "letâ€™s\n",
      "â˜¹ï¸\n",
      "ğŸ™Œ\n",
      "dâ€‘'\n",
      "ğŸ™Š\n",
      "ğŸ¤¦â€\n",
      "wlcm\n",
      "ğŸ‘»\n",
      "ğŸ‘ğŸ»\n",
      "ğŸ’\n",
      "ğŸ’›\n",
      "ğŸ˜—\n",
      "it'\n",
      "ğŸ˜µ\n",
      "iâ€™ll\n",
      "ğŸ˜¼\n",
      "ğŸŒ¹\n",
      "man's\n",
      "mom's\n",
      "how've\n",
      "ğŸ’œ\n",
      "ğŸ‘‹\n",
      "ğŸ’“\n",
      "ğŸ‘©â€\n",
      "wht's\n",
      "ohkk\n",
      "ğŸ’ƒ\n",
      "ddlj\n",
      "mum's\n",
      "ğŸ’¤\n",
      "how'd\n",
      "ğŸ˜²\n",
      "âœŒï¸\n",
      "howâ€™s\n",
      "son's\n",
      "ğŸ»\n",
      "ğŸ¼\n",
      "ğŸ\n",
      "'6'\n",
      "'if'\n",
      "yrrr\n",
      "ğŸ’‘\n",
      "eh'\n",
      "swty\n",
      "â¤ï¸â€\n",
      "ğŸ‘¨\n",
      "ğŸ‘¨â€\n",
      "ğŸ’\n",
      "ğŸ±\n",
      "jzt\n",
      "i'n\n",
      "now'\n",
      "ğŸ™ƒ\n",
      "ğŸŒ\n",
      "lol'\n",
      "ğŸ’\n",
      "ğŸ¤˜\n",
      "ğŸ¤\n",
      "hwz\n",
      "ğŸš—\n",
      "okzz\n",
      "ğŸ‘…\n",
      "iÌ‡f\n",
      "ğŸ‘­\n",
      "â˜ï¸\n",
      "ğŸ‚\n",
      "ğŸ¾\n",
      "d'\n",
      "'rec'\n",
      "ai's\n",
      "qstn\n",
      "why's\n",
      "msgng\n",
      "dog's\n",
      "ğŸº\n",
      "eldg\n",
      "ğŸ¤¡\n",
      "ğŸ‘®\n",
      "ğŸ‘·\n",
      "iopp\n",
      "ğŸ”¥\n",
      "ma'am\n",
      "cry's\n",
      "guy's\n",
      "extc\n",
      "cllg\n",
      "ğŸ·\n",
      "hmmk\n",
      "ğŸ™ğŸ»\n",
      "'na'\n",
      "ğŸ‘ŠğŸ»\n",
      "tbhhh\n",
      "hmnn\n",
      "ğŸ\n",
      "ğŸ”\n",
      "dhf\n",
      "ğŸ‘¬\n",
      "ğŸ‘†\n",
      "itï¿½s\n",
      "vaijy\n",
      "ğŸ‘ğŸ¼\n",
      "ğŸ’­\n",
      "okz\n",
      "â‰§âˆ‡â‰¦\n",
      "ğŸ’©\n",
      "ukw\n",
      "ğŸ¤¦ğŸ»â€\n",
      "ğŸ‘¯\n",
      "ur's\n",
      "ğŸ†—\n",
      "ğŸ¶\n",
      "orâ€¦\n",
      "ğŸœ\n",
      "whoâ€™s\n",
      "ğŸ’ƒğŸ»\n",
      "ğŸ°\n",
      "'fake'\n",
      "cgat\n",
      "ğŸ’°\n",
      "ğŸº\n",
      "yyup\n",
      "ğŸ€\n",
      "bgst\n",
      "'noob'\n",
      "ğŸ’\n",
      "'i'm\n",
      "up'\n",
      "hapnd\n",
      "okhh\n",
      "ğŸ‘§\n",
      "Í¡Â°\n",
      "ğŸ‘ˆ\n",
      "lifw\n",
      "ğŸ™†ğŸ»\n",
      "ğŸ‘©\n",
      "ğŸ¤ª\n",
      "rudw\n",
      "gunyt\n",
      "haapy\n",
      "â“\n",
      "kxip\n",
      "ğŸ‘ğŸ¾\n",
      "i'l\n",
      "hol'\n",
      "lil'\n",
      "ğŸ™ğŸ¿\n",
      "nekde\n",
      "ğŸ–‘\n",
      "aadu2\n",
      "youï¿½re\n",
      "haooy\n",
      "Â¿did\n",
      "ğŸ‘¼ğŸ»\n",
      "whÃ©re\n",
      "ğŸš˜\n",
      "vkr\n",
      "â˜…â˜…â˜…\n",
      "how'z\n",
      "'ok'\n",
      "tqs\n",
      "1pi\n",
      "txtng\n",
      "mhfy\n",
      "huyokbfr\n",
      "jjftihc\n",
      "ytjph\n",
      "cbbbyb\n",
      "â›„\n",
      "'eid\n",
      "14dec\n",
      "s3nt\n",
      "sis'\n",
      "âš½ï¸\n",
      "ohkkk\n",
      "ğŸ·\n",
      "ğŸ°\n",
      "ğŸŒ¸\n",
      "woew\n",
      "hugr\n",
      "ğŸŒ\n",
      "ğŸ’\n",
      "ğŸŒ±\n",
      "mc'd\n",
      "so'\n",
      "ğŸ‘™\n",
      "ğŸ»\n",
      "q4a\n",
      "30's\n",
      "î„î„\n",
      "yo're\n",
      "ğŸ¡\n",
      "ğŸ\n",
      "ğŸ\n",
      "ğŸ¬\n",
      "ğŸ›\n",
      "yuw\n",
      "kehch\n",
      "rzn\n",
      "wdfhe\n",
      "whqt\n",
      "ğŸ™‹ğŸ»\n",
      "ğŸ‘µ\n",
      "ğŸ¶\n",
      "relxd\n",
      "ğŸ‘ğŸ¿\n",
      "ğŸ¶\n",
      "vkski\n",
      "wnaa\n",
      "god'\n",
      "madkolr\n",
      "meÂ¿\n",
      "bayys\n",
      "rp's\n",
      "ğŸšŒ\n",
      "leqrn\n",
      "yeaj\n",
      "krle\n",
      "jug4\n",
      "pic's\n",
      "itÓ³\n",
      "ozdgoisrgkfgj\n",
      "dkfjkjdfngrfdskfdfjlnfnfef\n",
      "kbai\n",
      "tqsm\n",
      "ppz\n",
      "00e\n",
      "youï¼\n",
      "juzz\n",
      "gdmg\n",
      "khjjfk\n",
      "a'm\n",
      "ğŸ›°\n",
      "viju\n",
      "yhh\n",
      "Â¡so\n",
      "frirnd\n",
      "ywah\n",
      "ğŸ¨\n",
      "vh2o\n",
      "xdxd\n",
      "god's\n",
      "ğŸ­\n",
      "ğŸ‘‰\n",
      "ğŸ‘«\n",
      "okf\n",
      "gdxkgasgc\n",
      "ishh\n",
      "oghh\n",
      "whyâ€™s\n",
      "ğŸš©\n",
      "â€œwaysâ€\n",
      "lot's\n",
      "job'\n",
      "rybgrbi\n",
      "trru\n",
      "riyl\n",
      "gvng\n",
      "yhaa\n",
      "do't\n",
      "ohhkk\n",
      "mjnd\n",
      "ğŸ¤‘\n",
      "daaw\n",
      "gudn8\n",
      "ğŸ‘½\n",
      "ni8\n",
      "sadâ€¦\n",
      "ğŸŒ\n",
      "vxxvgfeecbko\n",
      "ttyt\n",
      "go'\n",
      "hapn\n",
      "klll\n",
      "ğŸ‘§â€\n",
      "bff's\n",
      "ğŸ¤¥\n",
      "ğŸ¤•\n",
      "prsn\n",
      "ğŸ™\n",
      "aiyyoo\n",
      "djxjv\n",
      "chxk\n",
      "rÅŸch\n",
      "mkhi\n",
      "abÄ±t\n",
      "ireq\n",
      "99myh1orbs4\n",
      "tuuh\n",
      "tkqa\n",
      "hfdgj\n",
      "'i'll\n",
      "amn't\n",
      "cn't\n",
      "al's\n",
      "luok\n",
      "cmng\n",
      "uhhuh\n",
      "babt\n",
      "ni8t\n",
      "ainvy\n",
      "pixx\n",
      "ğŸ—\n",
      "kufe\n",
      "pucy\n",
      "humr\n",
      "â›ªï¸\n",
      "ğŸ•º\n",
      "isâ€¦\n",
      "donÓµ\n",
      "pweh\n",
      "'koe\n",
      "ğŸ”±\n",
      "dky\n",
      "ğŸ­\n",
      "ğŸ¹\n",
      "ğŸ•\n",
      "kdjhffjdkkd\n",
      "y'all's\n",
      "nmbr\n",
      "ğŸŒŸ\n",
      "epabe\n",
      "wrud\n",
      "ğŸ‘¶ğŸ½\n",
      "'05\n",
      "âœ‹\n",
      "c0ol\n",
      "stdnt\n",
      "ohhu\n",
      "oglf\n",
      "th8s\n",
      "ğŸ”™\n",
      "ğŸ”œ\n",
      "5'6\n",
      "laae\n",
      "whttt\n",
      "bu5\n",
      "â”€à¹‘\n",
      "odrs\n",
      "meâ€¦\n",
      "aaap\n",
      "ğŸš¦\n",
      "70's\n",
      "nilk\n",
      "âœ”ï¸\n",
      "ohhooâ€¦all\n",
      "ğŸ–•ğŸ»\n",
      "b9red\n",
      "wipl\n",
      "yrf's\n",
      "daznt\n",
      "ğŸ‘¶ğŸ»\n",
      "'top\n",
      "ywss\n",
      "wwll\n",
      "ğŸƒğŸ»\n",
      "ğŸ™\n",
      "ktÃ³w\n",
      "srap\n",
      "ğŸ¼\n",
      "bsdk\n",
      "â‚¹1\n",
      "lucj\n",
      "amã€‚so\n",
      "bkl\n",
      "paib\n",
      "leyy\n",
      "â‚¹100\n",
      "amy'\n",
      "lstm\n",
      "yrf\n",
      "it'a\n",
      "gb's\n",
      "hwll\n",
      "'rare'\n",
      "dhrr\n",
      "fhw\n",
      "dyrtdd\n",
      "gfvsg\n",
      "rddsw\n",
      "hrll\n",
      "'ex'\n",
      "âœˆï¸\n",
      "ye'r\n",
      "ğŸ“\n",
      "gt70\n",
      "'bro'\n",
      "mooz\n",
      "jstu\n",
      "yrss\n",
      "kbye\n",
      "naaqs\n",
      "hoezaay\n",
      "cncrn\n",
      "ğŸ‡\n",
      "locx\n",
      "tbbt\n",
      "'89\n",
      "ahuh\n",
      "uffd\n",
      "lokl\n",
      "10'ok\n",
      "ğŸ“²\n",
      "ğŸ§\n",
      "ğŸ£\n",
      "âš˜âš˜âš˜âš˜âš˜âš˜âš˜âš˜âš˜âš˜âš˜âš˜âš˜âš˜âš˜âš˜âš˜âš˜âš˜âš˜\n",
      "ğŸŒ·\n",
      "tnzz\n",
      "ğŸ¤’\n",
      "ğŸµ\n",
      "i'ts\n",
      "ğŸ’ƒğŸ¼\n",
      "bakbus\n",
      "ğŸ‘ª\n",
      "y08\n",
      "abtbur\n",
      "mb20\n",
      "ğŸƒ\n",
      "ğŸ‘ğŸ»\n",
      "wrrr\n",
      "â‰ï¸\n",
      "ğŸ¤·ğŸ»â€\n",
      "ğŸ‘‡\n",
      "faje\n",
      "ğŸ‘º\n",
      "wprd\n",
      "jokr\n",
      "ÍœÊ–\n",
      "ğŸ‰\n",
      "iitb\n",
      "nehh\n",
      "72e\n",
      "bchfhmnvcffgghhjkhhggfvb\n",
      "ğŸ»\n",
      "ruih\n",
      "ğŸ¤¢\n",
      "ğŸ“†\n",
      "thxxx\n",
      "iï¿½m\n",
      "ğŸ˜\n",
      "ğŸ‘¼\n",
      "wlcmmm\n",
      "yeyy\n",
      "xddd\n",
      "'lol'\n",
      "chtbt\n",
      "ğŸ‘—\n",
      "'3\n",
      "ğŸ§\n",
      "âœŒğŸ»\n",
      "fiÌ‡nd\n",
      "ğŸ–\n",
      "wtz\n",
      "ğŸ”ª\n",
      "ğŸ \n",
      "awwn\n",
      "ğŸ’‹â€\n",
      "'eeee'\n",
      "fvrt\n",
      "jo'\n",
      "ol'\n",
      "hww\n",
      "kkbye\n",
      "ğŸ–•ğŸ¼\n",
      "â˜ğŸ»ï¸\n",
      "huuh\n",
      "'hmm\n",
      "k'\n",
      "kÃ±ew\n",
      "lexgoww\n",
      "l0o\n",
      "tlks\n",
      "ğŸ‘ğŸ½\n",
      "naxt\n",
      "bbhe\n",
      "ğŸ‘‹ğŸ»\n",
      "5'8\n",
      "loivr\n",
      "hesrt\n",
      "slpy\n",
      "Ã§uz\n",
      "'saw'\n",
      "lol'd\n",
      "ğŸ‹\n",
      "ğŸ’ª\n",
      "gudni8\n",
      "q80\n",
      "canï¿½t\n",
      "utlr\n",
      "ğŸ’¯\n",
      "ğŸ‘\n",
      "ğŸ™‰\n",
      "thjnk\n",
      "ğŸš¬\n",
      "ğŸ˜™\n",
      "ğŸˆ\n",
      "6ilm\n",
      "ğŸ’â€\n",
      "â™€ï¸\n",
      "smwt\n",
      "xad\n",
      "v8sit\n",
      "chtng\n",
      "sebd\n",
      "daso\n",
      "ğŸƒ\n",
      "âª\n",
      "iooo\n",
      "dj's\n",
      "yfvx\n",
      "ğŸ’¡\n",
      "ğŸ’µ\n",
      "qsts\n",
      "pllz\n",
      "ğŸ™‡\n",
      "'tu'\n",
      "btao\n",
      "kcubftv\n",
      "howâ€™re\n",
      "Äºove\n",
      "'bite'\n",
      "'dkla'\n",
      "topÄ±c\n",
      "iâ€¦\n",
      "hpnd\n",
      "lelk\n",
      "hÄ±mm\n",
      "oï¼›\n",
      "yoj\n",
      "ğŸ“\n",
      "hi555\n",
      "boq\n",
      "yuupss\n",
      "msgd\n",
      "q2b\n",
      "hshhshhhdjh\n",
      "dhjvfgj\n",
      "me'\n",
      "tenx\n",
      "wbuu\n",
      "dinr\n",
      "gn8\n",
      "hwru\n",
      "ğŸ¤–\n",
      "kkrh\n",
      "fhec\n",
      "s3nd\n",
      "'hmmm'\n",
      "okyyy\n",
      "20's\n",
      "ğŸ”Š\n",
      "ğŸ™ŒğŸ»\n",
      "oh'\n",
      "hhhffh\n",
      "iÌ‡t's\n",
      "6ep\n",
      "jaaju\n",
      "ğŸ‘‹ğŸ¼\n",
      "â¸ï¸\n",
      "15111\n",
      "Shape of training data tensor:  (30160, 100)\n",
      "Shape of label tensor:  (30160, 4)\n"
     ]
    }
   ],
   "source": [
    "    '''\n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    \n",
    "    Simple function calling.\n",
    "    \n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    '''\n",
    "    print(\"Processing training data...\")\n",
    "    trainIndices, trainTexts, labels = preprocessData(trainDataPath, mode=\"train\")\n",
    "    # Write normalised text to file to check if normalisation works. Disabled now. Uncomment following line to enable   \n",
    "    #writeNormalisedData(trainDataPath, trainTexts)\n",
    "    print(\"Processing test data...\")\n",
    "    testIndices, testTexts = preprocessData(testDataPath, mode=\"test\")\n",
    "    #writeNormalisedData(testDataPath, testTexts)\n",
    "\n",
    "    print(\"Extracting tokens...\")\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(trainTexts)\n",
    "    trainSequences = tokenizer.texts_to_sequences(trainTexts)\n",
    "    testSequences = tokenizer.texts_to_sequences(testTexts)\n",
    "\n",
    "    wordIndex = tokenizer.word_index\n",
    "    print(\"Found %s unique tokens.\" % len(wordIndex))\n",
    "\n",
    "    print(\"Populating embedding matrix...\")\n",
    "    embeddingMatrix = getEmbeddingMatrix(wordIndex)\n",
    "\n",
    "    data = pad_sequences(trainSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    labels = to_categorical(np.asarray(labels))\n",
    "    print(\"Shape of training data tensor: \", data.shape)\n",
    "    print(\"Shape of label tensor: \", labels.shape)\n",
    "        \n",
    "    # Randomize data\n",
    "    np.random.shuffle(trainIndices)\n",
    "    data = data[trainIndices]\n",
    "    labels = labels[trainIndices]\n",
    "      \n",
    "    # Perform k-fold cross validation\n",
    "    metrics = {\"accuracy\" : [],\n",
    "               \"microPrecision\" : [],\n",
    "               \"microRecall\" : [],\n",
    "               \"microF1\" : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    " '''\n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    \n",
    "    Build model\n",
    "    \n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    '''\n",
    "def buildModel(embeddingMatrix):\n",
    "    \"\"\"Constructs the architecture of the model\n",
    "    Input:\n",
    "        embeddingMatrix : The embedding matrix to be loaded in the embedding layer.\n",
    "    Output:\n",
    "        model : A basic LSTM model\n",
    "    \"\"\"\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(LSTM(LSTM_DIM, dropout=DROPOUT))\n",
    "    model.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "    \n",
    "    rmsprop = optimizers.rmsprop(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=rmsprop,\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
