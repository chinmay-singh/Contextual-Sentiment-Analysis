{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "import json, argparse, os\n",
    "import re\n",
    "import io\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataPath = \"train.txt\"\n",
    "testDataPath = \"devwithoutlabels.txt\"\n",
    "# Output file that will be generated. This file can be directly submitted.\n",
    "solutionPath = \"test.txt\"\n",
    "# Path to directory where GloVe file is saved.\n",
    "gloveDir = \"./\"\n",
    "NUM_FOLDS = 5                   # Value of K in K-fold Cross Validation\n",
    "NUM_CLASSES = 4                 # Number of classes - Happy, Sad, Angry, Others\n",
    "MAX_NB_WORDS = 20000                # To set the upper limit on the number of tokens extracted using keras.preprocessing.text.Tokenizer \n",
    "MAX_SEQUENCE_LENGTH = 100         # All sentences having lesser number of words than this will be padded\n",
    "EMBEDDING_DIM = 300               # The dimension of the word embeddings\n",
    "BATCH_SIZE = 200                  # The batch size to be chosen for training the model.\n",
    "LSTM_DIM = 128                    # The dimension of the representations learnt by the LSTM model\n",
    "DROPOUT = 0.2  \n",
    "LEARNING_RATE = 0.003 # Fraction of the units to drop for the linear transformation of the inputs. Ref - https://keras.io/layers/recurrent/\n",
    "NUM_EPOCHS = 75                  # Number of epochs to train a model for\n",
    "'''\n",
    "\n",
    "Change parameters here for the LSTM model used below\n",
    "\n",
    "'''\n",
    "\n",
    "label2emotion = {0:\"others\", 1:\"happy\", 2: \"sad\", 3:\"angry\"}\n",
    "emotion2label = {\"others\":0, \"happy\":1, \"sad\":2, \"angry\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(predictions, ground):\n",
    "    \"\"\"Given predicted labels and the respective ground truth labels, display some metrics\n",
    "    Input: shape [# of samples, NUM_CLASSES]\n",
    "        predictions : Model output. Every row has 4 decimal values, with the highest belonging to the predicted class\n",
    "        ground : Ground truth labels, converted to one-hot encodings. A sample belonging to Happy class will be [0, 1, 0, 0]\n",
    "    Output:\n",
    "        accuracy : Average accuracy\n",
    "        microPrecision : Precision calculated on a micro level. Ref - https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/16001\n",
    "        microRecall : Recall calculated on a micro level\n",
    "        microF1 : Harmonic mean of microPrecision and microRecall. Higher value implies better classification  \n",
    "    \"\"\"\n",
    "    # [0.1, 0.3 , 0.2, 0.1] -> [0, 1, 0, 0]\n",
    "    discretePredictions = to_categorical(predictions.argmax(axis=1))\n",
    "    \n",
    "    truePositives = np.sum(discretePredictions*ground, axis=0)\n",
    "    falsePositives = np.sum(np.clip(discretePredictions - ground, 0, 1), axis=0)\n",
    "    falseNegatives = np.sum(np.clip(ground-discretePredictions, 0, 1), axis=0)\n",
    "    '''\n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    \n",
    "    IMPORTANT\n",
    "    Please add  code here or somewhere else to print the TruePositives,FalsePositives,FalseNegatives\n",
    "    \n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    '''\n",
    "    \n",
    "    print(\"True Positives per class : \", truePositives)\n",
    "    print(\"False Positives per class : \", falsePositives)\n",
    "    print(\"False Negatives per class : \", falseNegatives)\n",
    "    \n",
    "    # ------------- Macro level calculation ---------------\n",
    "    macroPrecision = 0\n",
    "    macroRecall = 0\n",
    "    # We ignore the \"Others\" class during the calculation of Precision, Recall and F1\n",
    "    for c in range(1, NUM_CLASSES):\n",
    "        precision = truePositives[c] / (truePositives[c] + falsePositives[c])\n",
    "        macroPrecision += precision\n",
    "        recall = truePositives[c] / (truePositives[c] + falseNegatives[c])\n",
    "        macroRecall += recall\n",
    "        f1 = ( 2 * recall * precision ) / (precision + recall) if (precision+recall) > 0 else 0\n",
    "        print(\"Class %s : Precision : %.3f, Recall : %.3f, F1 : %.3f\" % (label2emotion[c], precision, recall, f1))\n",
    "    \n",
    "    macroPrecision /= 3\n",
    "    macroRecall /= 3\n",
    "    macroF1 = (2 * macroRecall * macroPrecision ) / (macroPrecision + macroRecall) if (macroPrecision+macroRecall) > 0 else 0\n",
    "    print(\"Ignoring the Others class, Macro Precision : %.4f, Macro Recall : %.4f, Macro F1 : %.4f\" % (macroPrecision, macroRecall, macroF1))   \n",
    "    \n",
    "    # ------------- Micro level calculation ---------------\n",
    "    truePositives = truePositives[1:].sum()\n",
    "    falsePositives = falsePositives[1:].sum()\n",
    "    falseNegatives = falseNegatives[1:].sum()    \n",
    "    \n",
    "    print(\"Ignoring the Others class, Micro TP : %d, FP : %d, FN : %d\" % (truePositives, falsePositives, falseNegatives))\n",
    "    \n",
    "    microPrecision = truePositives / (truePositives + falsePositives)\n",
    "    microRecall = truePositives / (truePositives + falseNegatives)\n",
    "    \n",
    "    microF1 = ( 2 * microRecall * microPrecision ) / (microPrecision + microRecall) if (microPrecision+microRecall) > 0 else 0\n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    ground = ground.argmax(axis=1)\n",
    "    accuracy = np.mean(predictions==ground)\n",
    "    \n",
    "    print(\"Accuracy : %.4f, Micro Precision : %.4f, Micro Recall : %.4f, Micro F1 : %.4f\" % (accuracy, microPrecision, microRecall, microF1))\n",
    "    return accuracy, microPrecision, microRecall, microF1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeNormalisedData(dataFilePath, texts):\n",
    "    \"\"\"Write normalised data to a file\n",
    "    Input:\n",
    "        dataFilePath : Path to original train/test file that has been processed\n",
    "        texts : List containing the normalised 3 turn conversations, separated by the <eos> tag.\n",
    "    \"\"\"\n",
    "    \n",
    "    '''\n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    \n",
    "    You May ignore this function\n",
    "    \n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    '''\n",
    "    normalisedDataFilePath = dataFilePath.replace(\".txt\", \"_normalised.txt\")\n",
    "    with io.open(normalisedDataFilePath, 'w', encoding='utf8') as fout:\n",
    "        with io.open(dataFilePath, encoding='utf8') as fin:\n",
    "            fin.readline()\n",
    "            for lineNum, line in enumerate(fin):\n",
    "                line = line.strip().split('\\t')\n",
    "                normalisedLine = texts[lineNum].strip().split('<eos>')\n",
    "                fout.write(line[0] + '\\t')\n",
    "                # Write the original turn, followed by the normalised version of the same turn\n",
    "                fout.write(line[1] + '\\t' + normalisedLine[0] + '\\t')\n",
    "                fout.write(line[2] + '\\t' + normalisedLine[1] + '\\t')\n",
    "                fout.write(line[3] + '\\t' + normalisedLine[2] + '\\t')\n",
    "                try:\n",
    "                    # If label information available (train time)\n",
    "                    fout.write(line[4] + '\\n')    \n",
    "                except:\n",
    "                    # If label information not available (test time)\n",
    "                    fout.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    ##########################################################\\n    ##########################################################\\n    \\n    Fasttext model being loaded here. It might take 1 min to load.\\n    This bin file is of 10mb you can look for 5 gb variant of it.\\n    Also download the 300d English language '.bin' file not '.vec' file.\\n    \\n    ##########################################################\\n    ##########################################################\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.wrappers import FastText\n",
    "mod = FastText.load_fasttext_format('/home/bt1/17CS10037/taddhita/cc.en.300.bin', encoding=\"utf8\")\n",
    "\n",
    "\n",
    "'''\n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    \n",
    "    Fasttext model being loaded here. It might take 1 min to load.\n",
    "    This bin file is of 10mb you can look for 5 gb variant of it.\n",
    "    Also download the 300d English language '.bin' file not '.vec' file.\n",
    "    \n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix(wordIndex):\n",
    "    \"\"\"Populate an embedding matrix using a word-index. If the word \"happy\" has an index 19,\n",
    "       the 19th row in the embedding matrix should contain the embedding vector for the word \"happy\".\n",
    "    Input:\n",
    "        wordIndex : A dictionary of (word : index) pairs, extracted using a tokeniser\n",
    "    Output:\n",
    "        embeddingMatrix : A matrix where every row has 100 dimensional GloVe embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    # Minimum word index of any word is 1. \n",
    "    '''\n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    \n",
    "    If you want to use gloVe model you may use but main purpose of it is to generate wordtovec\n",
    "    If possible look up for some better library than fasttext.bin file here which can deal with emoji as well\n",
    "    \n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    '''\n",
    "    i=0\n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, EMBEDDING_DIM))\n",
    "    for word, i in wordIndex.items():\n",
    "        try:\n",
    "            embeddingVector = mod[word]\n",
    "            embeddingMatrix[i] = embeddingVector    \n",
    "        except KeyError:\n",
    "            print(word)\n",
    "            i=i+1\n",
    "    print(i)\n",
    "    return embeddingMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import regex\n",
    "\n",
    "def split_count(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['😍', '😍']\n"
     ]
    }
   ],
   "source": [
    "line = [\"🤔 🙈 me así, se 😌 ds 💕👭👙 hello 👩🏾‍🎓 emoji hello 👨‍👩‍👦‍👦 how are 😊 you today🙅🏽🙅🏽\"]\n",
    "line1=[\"money money and lots of money😍😍\"]\n",
    "\n",
    "counter = split_count(line1[0])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(dataFilePath, mode):\n",
    "    \"\"\"Load data from a file, process and return indices, conversations and labels in separate lists\n",
    "    Input:\n",
    "        dataFilePath : Path to train/test file to be processed\n",
    "        mode : \"train\" mode returns labels. \"test\" mode doesn't return labels.\n",
    "    Output:\n",
    "        indices : Unique conversation ID list\n",
    "        conversations : List of 3 turn conversations, processed and each turn separated by the <eos> tag\n",
    "        labels : [Only available in \"train\" mode] List of labels\n",
    "    \"\"\"\n",
    "    \n",
    "    '''\n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    \n",
    "    IMPORTANT\n",
    "    Please try to better the pre-processing here by sepearting the emojis with text like  \"hmm😞\", \"now‼‼‼😣😤😤\" & etc.\n",
    "    Also look for other methods.\n",
    "    \n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    '''\n",
    "    indices = []\n",
    "    conversations = []\n",
    "    labels = []\n",
    "    i=0\n",
    "    with io.open(dataFilePath, encoding=\"utf8\") as finput:\n",
    "        finput.readline()\n",
    "        for line in finput:\n",
    "            i+=1\n",
    "            # Convert multiple instances of . ? ! , to single instance\n",
    "            # okay...sure -> okay . sure\n",
    "            # okay???sure -> okay ? sure\n",
    "            # Add whitespace around such punctuation\n",
    "            # okay!sure -> okay ! sure\n",
    "            emojis = split_count(line)\n",
    "            repeatedChars = ['.']\n",
    "            repeatedChars.append('?')\n",
    "            repeatedChars.append(',')\n",
    "                           \n",
    "            for c in repeatedChars:\n",
    "                lineSplit = line.split(c)\n",
    "                while True:\n",
    "                    try:\n",
    "                        lineSplit.remove('')\n",
    "                    except:\n",
    "                        break\n",
    "                cSpace = ' ' + c + ' '    \n",
    "                line = cSpace.join(lineSplit)\n",
    "            for c in emojis:\n",
    "                lineSplit = line.split(c)\n",
    "                while True:\n",
    "                    try:\n",
    "                        lineSplit.remove('')\n",
    "                    except:\n",
    "                        break\n",
    "                cSpace = ' ' + c + ' '    \n",
    "                line = cSpace.join(lineSplit)\n",
    "            \n",
    "            line = line.strip().split('\\t')\n",
    "            if mode == \"train\":\n",
    "                # Train data contains id, 3 turns and label\n",
    "                label = emotion2label[line[4]]\n",
    "                labels.append(label)\n",
    "            \n",
    "            conv = ' <eos> '.join(line[1:4])\n",
    "            \n",
    "            # Remove any duplicate spaces\n",
    "            duplicateSpacePattern = re.compile(r'\\ +')\n",
    "            conv = re.sub(duplicateSpacePattern, ' ', conv)\n",
    "            \n",
    "            indices.append(int(line[0]))\n",
    "            conversations.append(conv.lower())\n",
    "    if mode == \"train\":\n",
    "        return indices, conversations, labels\n",
    "    else:\n",
    "        return indices, conversations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Processing test data...\n",
      "Extracting tokens...\n",
      "Found 15111 unique tokens.\n",
      "Populating embedding matrix...\n",
      "😂\n",
      "😭\n",
      "i'll\n",
      "😞\n",
      "😁\n",
      "😢\n",
      "😍\n",
      "😀\n",
      "😡\n",
      "😅\n",
      "i've\n",
      "😆\n",
      "😄\n",
      "😊\n",
      "😃\n",
      "😒\n",
      "😠\n",
      "🙂\n",
      "😤\n",
      "😉\n",
      "😘\n",
      "😌\n",
      "😹\n",
      "😺\n",
      "how's\n",
      "😩\n",
      "i'd\n",
      "😸\n",
      "😫\n",
      "👍\n",
      "i’m\n",
      "💔\n",
      "😻\n",
      "😽\n",
      "😑\n",
      "😋\n",
      "😝\n",
      "😔\n",
      "❤️\n",
      "😏\n",
      "🙁\n",
      "😬\n",
      "ain't\n",
      "😾\n",
      "🙀\n",
      "😿\n",
      "😧\n",
      "😦\n",
      "🙄\n",
      "😛\n",
      "👎\n",
      "😐\n",
      "😖\n",
      "😥\n",
      "🎁\n",
      "🤣\n",
      "😇\n",
      "‑d\n",
      "😕\n",
      "☺️\n",
      "💕\n",
      "😪\n",
      "😣\n",
      "'‑\n",
      "👌\n",
      "🤔\n",
      "😶\n",
      "😟\n",
      "🙈\n",
      "🙏\n",
      "😱\n",
      "😳\n",
      "i̇\n",
      "8‑d\n",
      "x‑d\n",
      "😯\n",
      "😰\n",
      "💙\n",
      "😓\n",
      "😨\n",
      "😮\n",
      "okh\n",
      "💋\n",
      "🤗\n",
      "😈\n",
      "💞\n",
      "how're\n",
      "🖕\n",
      "♂️\n",
      "😴\n",
      "󾌹\n",
      "👄\n",
      "✨\n",
      "💚\n",
      "it'll\n",
      "😷\n",
      "‑c\n",
      "i’ve\n",
      "👏\n",
      "💖\n",
      "‑3\n",
      "👀\n",
      "：）\n",
      "・ω・\n",
      "u'r\n",
      "💗\n",
      "ohhk\n",
      "👦\n",
      "👿\n",
      "😚\n",
      "💐\n",
      "byyy\n",
      "💘\n",
      "👊\n",
      "it'd\n",
      "nthg\n",
      "hbu\n",
      "let’s\n",
      "☹️\n",
      "🙌\n",
      "d‑'\n",
      "🙊\n",
      "🤦‍\n",
      "wlcm\n",
      "👻\n",
      "👍🏻\n",
      "💝\n",
      "💛\n",
      "😗\n",
      "it'\n",
      "😵\n",
      "i’ll\n",
      "😼\n",
      "🌹\n",
      "man's\n",
      "mom's\n",
      "how've\n",
      "💜\n",
      "👋\n",
      "💓\n",
      "👩‍\n",
      "wht's\n",
      "ohkk\n",
      "💃\n",
      "ddlj\n",
      "mum's\n",
      "💤\n",
      "how'd\n",
      "😲\n",
      "✌️\n",
      "how’s\n",
      "son's\n",
      "🐻\n",
      "🐼\n",
      "🐍\n",
      "'6'\n",
      "'if'\n",
      "yrrr\n",
      "💑\n",
      "eh'\n",
      "swty\n",
      "❤️‍\n",
      "👨\n",
      "👨‍\n",
      "💍\n",
      "🐱\n",
      "jzt\n",
      "i'n\n",
      "now'\n",
      "🙃\n",
      "🌍\n",
      "lol'\n",
      "🍒\n",
      "🤘\n",
      "🤐\n",
      "hwz\n",
      "🚗\n",
      "okzz\n",
      "👅\n",
      "i̇f\n",
      "👭\n",
      "☝️\n",
      "🎂\n",
      "🍾\n",
      "d'\n",
      "'rec'\n",
      "ai's\n",
      "qstn\n",
      "why's\n",
      "msgng\n",
      "dog's\n",
      "🐺\n",
      "eldg\n",
      "🤡\n",
      "👮\n",
      "👷\n",
      "iopp\n",
      "🔥\n",
      "ma'am\n",
      "cry's\n",
      "guy's\n",
      "extc\n",
      "cllg\n",
      "🐷\n",
      "hmmk\n",
      "🙏🏻\n",
      "'na'\n",
      "👊🏻\n",
      "tbhhh\n",
      "hmnn\n",
      "🍞\n",
      "🐔\n",
      "dhf\n",
      "👬\n",
      "👆\n",
      "it�s\n",
      "vaijy\n",
      "👍🏼\n",
      "💭\n",
      "okz\n",
      "≧∇≦\n",
      "💩\n",
      "ukw\n",
      "🤦🏻‍\n",
      "👯\n",
      "ur's\n",
      "🆗\n",
      "🐶\n",
      "or…\n",
      "🍜\n",
      "who’s\n",
      "💃🏻\n",
      "🐰\n",
      "'fake'\n",
      "cgat\n",
      "💰\n",
      "🍺\n",
      "yyup\n",
      "🏀\n",
      "bgst\n",
      "'noob'\n",
      "🐒\n",
      "'i'm\n",
      "up'\n",
      "hapnd\n",
      "okhh\n",
      "👧\n",
      "͡°\n",
      "👈\n",
      "lifw\n",
      "🙆🏻\n",
      "👩\n",
      "🤪\n",
      "rudw\n",
      "gunyt\n",
      "haapy\n",
      "❓\n",
      "kxip\n",
      "👏🏾\n",
      "i'l\n",
      "hol'\n",
      "lil'\n",
      "🙏🏿\n",
      "nekde\n",
      "🖑\n",
      "aadu2\n",
      "you�re\n",
      "haooy\n",
      "¿did\n",
      "👼🏻\n",
      "whére\n",
      "🚘\n",
      "vkr\n",
      "★★★\n",
      "how'z\n",
      "'ok'\n",
      "tqs\n",
      "1pi\n",
      "txtng\n",
      "mhfy\n",
      "huyokbfr\n",
      "jjftihc\n",
      "ytjph\n",
      "cbbbyb\n",
      "⛄\n",
      "'eid\n",
      "14dec\n",
      "s3nt\n",
      "sis'\n",
      "⚽️\n",
      "ohkkk\n",
      "🍷\n",
      "🍰\n",
      "🌸\n",
      "woew\n",
      "hugr\n",
      "🌞\n",
      "💎\n",
      "🌱\n",
      "mc'd\n",
      "so'\n",
      "👙\n",
      "🍻\n",
      "q4a\n",
      "30's\n",
      "\n",
      "yo're\n",
      "🏡\n",
      "🐞\n",
      "🐝\n",
      "🐬\n",
      "🐛\n",
      "yuw\n",
      "kehch\n",
      "rzn\n",
      "wdfhe\n",
      "whqt\n",
      "🙋🏻\n",
      "👵\n",
      "🍶\n",
      "relxd\n",
      "👏🏿\n",
      "🎶\n",
      "vkski\n",
      "wnaa\n",
      "god'\n",
      "madkolr\n",
      "me¿\n",
      "bayys\n",
      "rp's\n",
      "🚌\n",
      "leqrn\n",
      "yeaj\n",
      "krle\n",
      "jug4\n",
      "pic's\n",
      "itӳ\n",
      "ozdgoisrgkfgj\n",
      "dkfjkjdfngrfdskfdfjlnfnfef\n",
      "kbai\n",
      "tqsm\n",
      "ppz\n",
      "00e\n",
      "you！\n",
      "juzz\n",
      "gdmg\n",
      "khjjfk\n",
      "a'm\n",
      "🛰\n",
      "viju\n",
      "yhh\n",
      "¡so\n",
      "frirnd\n",
      "ywah\n",
      "🐨\n",
      "vh2o\n",
      "xdxd\n",
      "god's\n",
      "🍭\n",
      "👉\n",
      "👫\n",
      "okf\n",
      "gdxkgasgc\n",
      "ishh\n",
      "oghh\n",
      "why’s\n",
      "🚩\n",
      "“ways”\n",
      "lot's\n",
      "job'\n",
      "rybgrbi\n",
      "trru\n",
      "riyl\n",
      "gvng\n",
      "yhaa\n",
      "do't\n",
      "ohhkk\n",
      "mjnd\n",
      "🤑\n",
      "daaw\n",
      "gudn8\n",
      "👽\n",
      "ni8\n",
      "sad…\n",
      "🍌\n",
      "vxxvgfeecbko\n",
      "ttyt\n",
      "go'\n",
      "hapn\n",
      "klll\n",
      "👧‍\n",
      "bff's\n",
      "🤥\n",
      "🤕\n",
      "prsn\n",
      "🙍\n",
      "aiyyoo\n",
      "djxjv\n",
      "chxk\n",
      "rşch\n",
      "mkhi\n",
      "abıt\n",
      "ireq\n",
      "99myh1orbs4\n",
      "tuuh\n",
      "tkqa\n",
      "hfdgj\n",
      "'i'll\n",
      "amn't\n",
      "cn't\n",
      "al's\n",
      "luok\n",
      "cmng\n",
      "uhhuh\n",
      "babt\n",
      "ni8t\n",
      "ainvy\n",
      "pixx\n",
      "🍗\n",
      "kufe\n",
      "pucy\n",
      "humr\n",
      "⛪️\n",
      "🕺\n",
      "is…\n",
      "donӵ\n",
      "pweh\n",
      "'koe\n",
      "🔱\n",
      "dky\n",
      "🐭\n",
      "🐹\n",
      "🏕\n",
      "kdjhffjdkkd\n",
      "y'all's\n",
      "nmbr\n",
      "🌟\n",
      "epabe\n",
      "wrud\n",
      "👶🏽\n",
      "'05\n",
      "✋\n",
      "c0ol\n",
      "stdnt\n",
      "ohhu\n",
      "oglf\n",
      "th8s\n",
      "🔙\n",
      "🔜\n",
      "5'6\n",
      "laae\n",
      "whttt\n",
      "bu5\n",
      "─๑\n",
      "odrs\n",
      "me…\n",
      "aaap\n",
      "🚦\n",
      "70's\n",
      "nilk\n",
      "✔️\n",
      "ohhoo…all\n",
      "🖕🏻\n",
      "b9red\n",
      "wipl\n",
      "yrf's\n",
      "daznt\n",
      "👶🏻\n",
      "'top\n",
      "ywss\n",
      "wwll\n",
      "🏃🏻\n",
      "🐙\n",
      "któw\n",
      "srap\n",
      "🍼\n",
      "bsdk\n",
      "₹1\n",
      "lucj\n",
      "am。so\n",
      "bkl\n",
      "paib\n",
      "leyy\n",
      "₹100\n",
      "amy'\n",
      "lstm\n",
      "yrf\n",
      "it'a\n",
      "gb's\n",
      "hwll\n",
      "'rare'\n",
      "dhrr\n",
      "fhw\n",
      "dyrtdd\n",
      "gfvsg\n",
      "rddsw\n",
      "hrll\n",
      "'ex'\n",
      "✈️\n",
      "ye'r\n",
      "🐓\n",
      "gt70\n",
      "'bro'\n",
      "mooz\n",
      "jstu\n",
      "yrss\n",
      "kbye\n",
      "naaqs\n",
      "hoezaay\n",
      "cncrn\n",
      "🐇\n",
      "locx\n",
      "tbbt\n",
      "'89\n",
      "ahuh\n",
      "uffd\n",
      "lokl\n",
      "10'ok\n",
      "📲\n",
      "🎧\n",
      "🏣\n",
      "⚘⚘⚘⚘⚘⚘⚘⚘⚘⚘⚘⚘⚘⚘⚘⚘⚘⚘⚘⚘\n",
      "🌷\n",
      "tnzz\n",
      "🤒\n",
      "🎵\n",
      "i'ts\n",
      "💃🏼\n",
      "bakbus\n",
      "👪\n",
      "y08\n",
      "abtbur\n",
      "mb20\n",
      "🏃\n",
      "👎🏻\n",
      "wrrr\n",
      "⁉️\n",
      "🤷🏻‍\n",
      "👇\n",
      "faje\n",
      "👺\n",
      "wprd\n",
      "jokr\n",
      "͜ʖ\n",
      "🎉\n",
      "iitb\n",
      "nehh\n",
      "72e\n",
      "bchfhmnvcffgghhjkhhggfvb\n",
      "🏻\n",
      "ruih\n",
      "🤢\n",
      "📆\n",
      "thxxx\n",
      "i�m\n",
      "🐘\n",
      "👼\n",
      "wlcmmm\n",
      "yeyy\n",
      "xddd\n",
      "'lol'\n",
      "chtbt\n",
      "👗\n",
      "'3\n",
      "🧐\n",
      "✌🏻\n",
      "fi̇nd\n",
      "🏖\n",
      "wtz\n",
      "🔪\n",
      "🐠\n",
      "awwn\n",
      "💋‍\n",
      "'eeee'\n",
      "fvrt\n",
      "jo'\n",
      "ol'\n",
      "hww\n",
      "kkbye\n",
      "🖕🏼\n",
      "☝🏻️\n",
      "huuh\n",
      "'hmm\n",
      "k'\n",
      "kñew\n",
      "lexgoww\n",
      "l0o\n",
      "tlks\n",
      "👍🏽\n",
      "naxt\n",
      "bbhe\n",
      "👋🏻\n",
      "5'8\n",
      "loivr\n",
      "hesrt\n",
      "slpy\n",
      "çuz\n",
      "'saw'\n",
      "lol'd\n",
      "🏋\n",
      "💪\n",
      "gudni8\n",
      "q80\n",
      "can�t\n",
      "utlr\n",
      "💯\n",
      "👐\n",
      "🙉\n",
      "thjnk\n",
      "🚬\n",
      "😙\n",
      "🎈\n",
      "6ilm\n",
      "💁‍\n",
      "♀️\n",
      "smwt\n",
      "xad\n",
      "v8sit\n",
      "chtng\n",
      "sebd\n",
      "daso\n",
      "🎃\n",
      "⏪\n",
      "iooo\n",
      "dj's\n",
      "yfvx\n",
      "💡\n",
      "💵\n",
      "qsts\n",
      "pllz\n",
      "🙇\n",
      "'tu'\n",
      "btao\n",
      "kcubftv\n",
      "how’re\n",
      "ĺove\n",
      "'bite'\n",
      "'dkla'\n",
      "topıc\n",
      "i…\n",
      "hpnd\n",
      "lelk\n",
      "hımm\n",
      "o；\n",
      "yoj\n",
      "🍓\n",
      "hi555\n",
      "boq\n",
      "yuupss\n",
      "msgd\n",
      "q2b\n",
      "hshhshhhdjh\n",
      "dhjvfgj\n",
      "me'\n",
      "tenx\n",
      "wbuu\n",
      "dinr\n",
      "gn8\n",
      "hwru\n",
      "🤖\n",
      "kkrh\n",
      "fhec\n",
      "s3nd\n",
      "'hmmm'\n",
      "okyyy\n",
      "20's\n",
      "🔊\n",
      "🙌🏻\n",
      "oh'\n",
      "hhhffh\n",
      "i̇t's\n",
      "6ep\n",
      "jaaju\n",
      "👋🏼\n",
      "⏸️\n",
      "15111\n",
      "Shape of training data tensor:  (30160, 100)\n",
      "Shape of label tensor:  (30160, 4)\n"
     ]
    }
   ],
   "source": [
    "    '''\n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    \n",
    "    Simple function calling.\n",
    "    \n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    '''\n",
    "    print(\"Processing training data...\")\n",
    "    trainIndices, trainTexts, labels = preprocessData(trainDataPath, mode=\"train\")\n",
    "    # Write normalised text to file to check if normalisation works. Disabled now. Uncomment following line to enable   \n",
    "    #writeNormalisedData(trainDataPath, trainTexts)\n",
    "    print(\"Processing test data...\")\n",
    "    testIndices, testTexts = preprocessData(testDataPath, mode=\"test\")\n",
    "    #writeNormalisedData(testDataPath, testTexts)\n",
    "\n",
    "    print(\"Extracting tokens...\")\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(trainTexts)\n",
    "    trainSequences = tokenizer.texts_to_sequences(trainTexts)\n",
    "    testSequences = tokenizer.texts_to_sequences(testTexts)\n",
    "\n",
    "    wordIndex = tokenizer.word_index\n",
    "    print(\"Found %s unique tokens.\" % len(wordIndex))\n",
    "\n",
    "    print(\"Populating embedding matrix...\")\n",
    "    embeddingMatrix = getEmbeddingMatrix(wordIndex)\n",
    "\n",
    "    data = pad_sequences(trainSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    labels = to_categorical(np.asarray(labels))\n",
    "    print(\"Shape of training data tensor: \", data.shape)\n",
    "    print(\"Shape of label tensor: \", labels.shape)\n",
    "        \n",
    "    # Randomize data\n",
    "    np.random.shuffle(trainIndices)\n",
    "    data = data[trainIndices]\n",
    "    labels = labels[trainIndices]\n",
    "      \n",
    "    # Perform k-fold cross validation\n",
    "    metrics = {\"accuracy\" : [],\n",
    "               \"microPrecision\" : [],\n",
    "               \"microRecall\" : [],\n",
    "               \"microF1\" : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    " '''\n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    \n",
    "    Build model\n",
    "    \n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    '''\n",
    "def buildModel(embeddingMatrix):\n",
    "    \"\"\"Constructs the architecture of the model\n",
    "    Input:\n",
    "        embeddingMatrix : The embedding matrix to be loaded in the embedding layer.\n",
    "    Output:\n",
    "        model : A basic LSTM model\n",
    "    \"\"\"\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(LSTM(LSTM_DIM, dropout=DROPOUT))\n",
    "    model.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "    \n",
    "    rmsprop = optimizers.rmsprop(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=rmsprop,\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
